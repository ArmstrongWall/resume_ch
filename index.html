<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />


<title>Johnny Wang | Robotics，Computer Vision</title>
<link href="css.css" rel="stylesheet" type="text/css" />
</head>

<body>
<div id="main">

	
		<div id="top-nav">
		
			<b>汪自强</b> | <small>Robotics Researcher</small>
	
		</div>

	
		<div id="header">
		
			<img src="images/header.png" alt="" width="770"/>
			
		</div>
		

	
		<div id="navigation" style="position: fixed">

			
			main menu
			
			<hr />
			
<a href="#Home" class="navigation">Home</a>
<a href="#Projects" class="navigation">Project Highlights</a>
<a href="#Publications" class="navigation">Publications</a>
		</div>
		

		<div id="content">
		
			<a name="Home"></a><h1>Welcome</h1>
			
			
    <p align="justify"> 
<img border="0" src="images/wzq.jpg" align="left" style="padding-right: 20px;padding-bottom: 20px"/>
机器人学爱好研究者， 主要专注于<b>机器人多传感器融合状态估计，同步建图与定位(SLAM)</b>. 2018年于同济大学获得控制科学与工程硕士学位，旋即加入驭势科技从事机器人定位系统工作。
</p><p> 
在此展示一些我撰写的论文、演示视频、开源程序和数据集等.
</p><p> 
<b>Links: </b>
<a href="https://github.com/ArmstrongWall"><img border="0" src="images/gh.png" height="13px"/> GitHub</a>


<br/>
<b>Contact: </b>
<a href="mailto:jajuengel@gmail.com">1531651@tongji.edu.cn</a>

<br/>
</p>






<br /><br /><br /><br />

			<a name="Projects"></a><h1>Project Highlights</h1>

<h2>Stereo-VI-DSO:双目直接稀疏视觉惯性里程计</h2>
<p align="justify"><b>Stereo-VI-DSO</b> 是我主导开发的视觉-惯性里程计(VIO)，使用双目相机和is a direct and sparse visual odometry method I developed, that combines the benefits of direct methods with those of sparse, point-based methods - greatly exceeding LSD-SLAM in runtime, accuracy and robustness. I developed DSO partly during my internship with Prof. Vladlen Koltun at Intel, and partly during my PhD at TU Munich with Prof. Daniel Cremers. You can find the <a href="https://github.com/JakobEngel/dso"><img border="0" src="images/gh.png" height="13px"/> open-source code on GitHub</a>. We also created the <b>TUM monoVO dataset</b>, 50 real-world, "out-of-the-lab" monocular sequences for benchmarking and evaluation <a href="https://vision.in.tum.de/data/datasets/mono-dataset"><img border="0" src="images/tum.png" height="13px"/> which can be found here</a>.
Development and extention of DSO is carried on in the TU Munich Computer Vision group, <a href="https://vision.in.tum.de/research/vslam/dso"><img border="0" src="images/tum.png" height="13px"/> see here</a>.
<br /><i>Original Publication (TPAMI)</i>: <a href="pdf/DSO.pdf"><img border="0" src="images/pdf.png" height="13px"/> [pdf]</a> (see <a href="#Publications">Publications</a> for more related papers)
<br /><br />
<embed src='http://player.youku.com/player.php/sid/XNDA5OTc1MjI1Ng==/v.swf' allowFullScreen='true' quality='high' width='480' height='400' align='middle' allowScriptAccess='always' type='application/x-shockwave-flash'></embed></p>
<br />


<h2>基于LSD-SLAM的机器人路径规划和轨迹跟踪系统</h2>
<p align="justify"><b>LSD-SLAM</b> is a semi-dense, direct SLAM method I developed during my PhD at TUM. It was based on a 
<a href="https://youtu.be/LZChzEcLNzI"><img border="0" src="images/yt.png" height="13px"/> semi-dense monocular odometry approach</a>, 
and - together with colleagues and students - we extended it to 
run <a href="https://youtu.be/X0hx2vxxTMg"><img border="0" src="images/yt.png" height="13px"/> in real-time on a smartphone</a>,
run <a href="https://youtu.be/oJt3Ln8H03s"><img border="0" src="images/yt.png" height="13px"/> with stereo cameras</a>, 
run as a <a href="https://youtu.be/XSvFpPYfKWA"><img border="0" src="images/yt.png" height="13px"/> tightly coupled visual-inertial odometry</a>, 
run <a href="https://youtu.be/v0NqMm7Q6S8"><img border="0" src="images/yt.png" height="13px"/> on omnidirectional cameras</a>, 
and even to be used for <a href="https://youtu.be/fWBsDwBJD-g"><img border="0" src="images/yt.png" height="13px"/> autonomously navigating a toy quadrocopter</a>.
For the original monocular version, we published <a href="https://github.com/tum-vision/lsd_slam"><img border="0" src="images/gh.png" height="13px"/> open-source code on GitHub</a>.
<br /><i>Original Publication (ECCV)</i>: <a href="pdf/engel14eccv.pdf"><img border="0" src="images/pdf.png" height="13px"/> [pdf]</a> (see <a href="#Publications">Publications</a> for more related papers)
<br />
<br />
<embed src='http://player.youku.com/player.php/sid/XNDA5OTc5MDEzMg==/v.swf' allowFullScreen='true' quality='high' width='480' height='400' align='middle' allowScriptAccess='always' type='application/x-shockwave-flash'></embed><br />
<br />
<br />

<h2>2013全国大学生Freesclae智能车竞赛</h2>
<p align="justify">For my Master's Thesis (also at TUM, with Juergen Sturm and Daniel Cremers), I worked on allowing a low-cost commodity quadrocopter (Parrot AR.Drone) fly autonomously in unknown environments, using <a href="http://www.robots.ox.ac.uk/~gk/PTAM/">PTAM</a>. The resulting system was demonstrated in public on many occasions; you can find the <a href="https://github.com/tum-vision/tum_ardrone"><img border="0" src="images/gh.png" height="13px"/> open-source code on GitHub</a>.
<br /><i>Original Publication (RAS)</i>: <a href="pdf/engel14ras.pdf"><img border="0" src="images/pdf.png" height="13px"/> [pdf]</a> (see <a href="#Publications">Publications</a> for more related papers)
<br />
<br />
<img border="0" src="images/7.jpg" align="left" width='480' height='400' style="padding-right: 20px;padding-bottom: 20px"/>
</p>


<br /><br />
<br /><br />
<a name="Publications"></a><h1>Publications</h1>

<p>
	<img border="0" src="pdf_img/engel17phd.png" align="left" width="80px" style="padding-right: 10px;padding-bottom: 10px"/>
	<b>Direct Sparse Visual-Inertial Odometry with Stereo Cameras</b> (Ziqiang Wang, Chengcheng Guo), <i>No open,under review by IROS</i>, 2019.
	<br />
	<a href="https://github.com/ArmstrongWall/Supplementary_Material_to_Stereo_VI_DSO/blob/master/Supplementary%20Material%20to%20Direct%20Sparse%20Visual-Inertial%20Odometry%20with%20Stereo%20Cameras.pdf"><img border="0" src="images/pdf.png" height="13px"/> [补充材料]</a>
	<a href="http://v.youku.com/v_show/id_XNDA5OTc1MjI1Ng==.html?spm=a2h3j.8428770.3416059.1"><img border="0" src="images/yt.png" height="13px"/> [视频]</a>
</p>


<p align=justify">
	<img border="0" src="pdf_img/engel2016dso.jpg" align="left" width="80px" style="padding-right: 10px;padding-bottom: 0px"/>
	<b>Wheeled Robots Path Planing and Tracking System Based on Monocular Visual SLAM</b> (Ziqiang Wang, Hegen Xu, Youwen Wan), 2018.
	<br />
	<a href="https://arxiv.org/abs/1807.06303"><img border="0" src="images/pdf.png" height="13px"/> [pdf]</a>
	<a href="https://youtu.be/C6-xwSOOdqQ"><img border="0" src="images/yt.png" height="13px"/> [video]</a>
	<a href="https://github.com/ArmstrongWall/lsd-slam"><img border="0" src="images/gh.png" height="13px"/> [code]</a>
</p>

<p>
	<img border="0" src="pdf_img/stumberg16exploration.png" align="left" width="80px" style="padding-right: 10px;padding-bottom: 20px"/>
	<b>Research and Implementation of Robot Path Planning Based on VSLAM</b> (Ziqiang Wang, Hegen Xu, Youwen Wan), In <i>International Conference on Electrical Engineering, Control and Robotics (EECR)</i>, 2018.
	<br />
	<a href="https://doi.org/10.1051/matecconf/201816006004"><img border="0" src="images/pdf.png" height="13px"/> [pdf]</a>
</p>




		</div>

		<div id="footer">

			<hr />

	Copyright 2019 | JohnnyWang | All Rights Reserved

		</div>


	
</div>

</body>

</html>
